{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom pathlib import Path\nimport os, re, shutil, sys\nfrom collections import Counter, defaultdict\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import sparse\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\n\nimport xgboost as xgb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TRAIN_FASTA = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_sequences.fasta\"\nTRAIN_TERMS = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_terms.tsv\"\nTRAIN_TAXONOMY = \"/kaggle/input/cafa-6-protein-function-prediction/Train/train_taxonomy.tsv\"\n\nTEST_FASTA  = \"/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset.fasta\"\nTEST_TAXON  = \"/kaggle/input/cafa-6-protein-function-prediction/Test/testsuperset-taxon-list.tsv\"\n\nIA_TSV      = \"/kaggle/input/cafa-6-protein-function-prediction/IA.tsv\"  # whitelist of GO terms\n\nOUT_BASE   = \"/kaggle/working/submission_baseline.tsv\"\nOUT_KAGGLE = \"/kaggle/working/submission.tsv\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================================\n# CAFA-6: XGBoost (OvR) Baseline with Kaggle GPU\n# - Features: TF-IDF (3-mer) + optional Taxonomy one-hot\n# - IA.tsv whitelist for allowed GO terms\n# - GPU if available (gpu_hist), else CPU fallback (hist)\n# - Generous emission (avoid 0.000 submissions)\n# - Auto-writes /kaggle/working/submission.tsv\n# ==========================================================\n\n\n# -------------------- PATHS --------------------\n\n\n# -------------------- PARAMS --------------------\n# Features\nKMER_K        = 3\nUSE_TAXONOMY  = True  # set False to skip taxonomy features\n\n# Label space control (important for runtime)\nMIN_COUNT     = 2       # keep GO terms appearing >= 2 times\nMAX_TERMS     = 1200    # cap number of GO classes (raise if your session allows)\n\n# Emission control (be generous; evaluator picks the threshold)\nMIN_PROB      = 1e-6\nTOP_K         = 200\nCAP_PER_PROT  = 1500\n\n# XGBoost params\nXGB_ROUNDS                = 200          # num_boost_round (increase if time allows)\nEARLY_STOPPING_ROUNDS     = 20\nMAX_DEPTH                 = 6\nETA                       = 0.1\nSUBSAMPLE                 = 0.8\nCOLSAMPLE_BYTREE          = 0.8\nLAMBDA_L2                 = 1.0\n\n# Training loop controls\nMIN_POSITIVES_PER_CLASS   = 5            # skip classes with too few positives\nVAL_SIZE                  = 0.1          # class-wise validation split for early stopping\nRANDOM_STATE              = 42\n\n# Speed sanity-check (set to False for real submission)\nDEBUG_SMALL   = False\nDEBUG_N_TRAIN = 1200     # #labelled proteins to use (train)\nDEBUG_N_TEST  = 600      # #test proteins to predict\n\n# -------------------- UTILITIES --------------------\ndef read_fasta_ids_and_seqs(path):\n    ids, seqs, cur_id, cur_seq = [], [], None, []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            if line.startswith(\">\"):\n                if cur_id is not None:\n                    ids.append(cur_id); seqs.append(\"\".join(cur_seq))\n                header = line[1:].strip()\n                cur_id = header.split(\"|\")[1].split()[0] if \"|\" in header else header.split()[0]\n                cur_seq = []\n            else:\n                cur_seq.append(line.strip())\n        if cur_id is not None:\n            ids.append(cur_id); seqs.append(\"\".join(cur_seq))\n    return ids, seqs\n\ndef load_train_terms(path):\n    df0 = pd.read_csv(path, sep=\"\\t\", dtype=str)\n    cols = [c.lower() for c in df0.columns]; df0.columns = cols\n    if \"entryid\" in cols and \"term\" in cols:\n        df = df0.rename(columns={\"entryid\":\"protein_id\",\"term\":\"go_id\"})[[\"protein_id\",\"go_id\"]]\n    else:\n        df = df0.iloc[:, :2].copy()\n        df.columns = [\"protein_id\",\"go_id\"]\n    return df.dropna().drop_duplicates()\n\ndef load_taxonomy(path):\n    df = pd.read_csv(path, sep=\"\\t\", header=None, dtype=str)\n    if df.shape[1] < 2:\n        raise ValueError(\"taxonomy TSV must have at least two columns.\")\n    df = df.iloc[:, :2].copy()\n    df.columns = [\"protein_id\",\"taxon\"]\n    return dict(zip(df[\"protein_id\"], df[\"taxon\"]))\n\ndef build_label_sets(df_terms):\n    mp = defaultdict(set)\n    for p,g in df_terms[[\"protein_id\",\"go_id\"]].itertuples(index=False):\n        mp[p].add(g)\n    return mp\n\ndef filter_terms_by_freq(p2t, min_count=2, max_terms=None):\n    c = Counter()\n    for ts in p2t.values(): c.update(ts)\n    items = [(g,n) for g,n in c.items() if n >= min_count]\n    items.sort(key=lambda x: x[1], reverse=True)\n    if max_terms: items = items[:max_terms]\n    return set(g for g,_ in items)\n\ndef load_allowed_go_terms(ia_path):\n    df = pd.read_csv(ia_path, sep=\"\\t\", dtype=str)\n    # Try to find a column that contains GO ids\n    go_cols = [c for c in df.columns if \"go\" in c.lower() or \"term\" in c.lower()]\n    if go_cols:\n        vals = df[go_cols[0]].dropna().astype(str).tolist()\n        return {v for v in vals if v.startswith(\"GO:\")}\n    # Fall back: scan all columns\n    s = set()\n    for col in df.columns:\n        s |= set(df[col].dropna().astype(str))\n    return {x for x in s if x.startswith(\"GO:\")}\n\ndef round_sig(x: float, sig=3) -> str:\n    if x <= 0: return None\n    s = f\"{x:.{sig}g}\"\n    return s if float(s) > 0 else \"0.001\"\n\ndef make_submission_rows(pids, gids, proba, min_prob=1e-6, top_k=200, cap=1500):\n    rows = []\n    for i, pid in enumerate(pids):\n        pr = proba[i]\n        idx = np.where(pr >= min_prob)[0]\n        if top_k: idx = np.unique(np.concatenate([idx, np.argsort(-pr)[:top_k]]))\n        idx = idx[np.argsort(-pr[idx])]\n        count = 0\n        for j in idx:\n            if count >= cap: break\n            sc = round_sig(pr[j], 3)\n            if sc: rows.append((pid, gids[j], sc)); count += 1\n    return rows\n\ndef get_xgb_device_params():\n    # Use GPU if available; else CPU\n    try:\n        import subprocess\n        has_gpu = False\n        try:\n            out = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n            has_gpu = (out.returncode == 0)\n        except Exception:\n            has_gpu = False\n        if has_gpu:\n            return dict(tree_method=\"gpu_hist\", predictor=\"gpu_predictor\")\n    except Exception:\n        pass\n    return dict(tree_method=\"hist\", predictor=\"auto\")\n\ndef train_xgb_gpu_binary(X, y, params, rounds=200, early_stopping=20, val_size=0.1, seed=42):\n    # Ensure we have both classes for validation\n    stratify = y if y.sum() > 0 and y.sum() < len(y) else None\n    X_tr, X_va, y_tr, y_va = train_test_split(\n        X, y, test_size=val_size, random_state=seed, stratify=stratify\n    )\n    dtr = xgb.DMatrix(X_tr, label=y_tr)\n    dva = xgb.DMatrix(X_va, label=y_va)\n    evals = [(dtr, \"train\"), (dva, \"valid\")]\n    bst = xgb.train(params, dtr, num_boost_round=rounds, evals=evals,\n                    early_stopping_rounds=early_stopping, verbose_eval=False)\n    return bst\n\n# -------------------- GPU INFO --------------------\nprint(\"XGBoost version:\", xgb.__version__)\ndev_params = get_xgb_device_params()\nprint(\"XGB device params:\", dev_params)\n\n# -------------------- LOAD DATA --------------------\nprint(\"Loading training FASTA...\")\ntrain_ids, train_seqs = read_fasta_ids_and_seqs(TRAIN_FASTA)\nprint(\"Loading training terms...\")\nterms_df = load_train_terms(TRAIN_TERMS)\nprot2terms_all = build_label_sets(terms_df)\n\n# Keep only proteins with labels and present in FASTA\nX_text, y_terms, used_ids = [], [], []\nfor pid, seq in zip(train_ids, train_seqs):\n    t = prot2terms_all.get(pid)\n    if t:\n        used_ids.append(pid); X_text.append(seq); y_terms.append(sorted(t))\nprint(\"Training instances used:\", len(used_ids))\n\nif DEBUG_SMALL:\n    n_keep = min(DEBUG_N_TRAIN, len(used_ids))\n    used_ids = used_ids[:n_keep]\n    X_text = X_text[:n_keep]\n    y_terms = y_terms[:n_keep]\n\n# Term filtering (frequency) and whitelist (IA.tsv)\nkept_freq = filter_terms_by_freq({pid:set(ts) for pid,ts in zip(used_ids, y_terms)},\n                                 min_count=MIN_COUNT, max_terms=MAX_TERMS)\nallowed_terms = load_allowed_go_terms(IA_TSV)\nprint(\"Kept by freq:\", len(kept_freq), \"| Allowed (IA):\", len(allowed_terms))\nkept_terms = kept_freq & allowed_terms\nprint(\"Kept after whitelist:\", len(kept_terms))\ny_filtered = [[t for t in ts if t in kept_terms] for ts in y_terms]\n\n# -------------------- FEATURES --------------------\nprint(\"Vectorizing sequences with TF-IDF(3-mer)...\")\nvect = TfidfVectorizer(analyzer=\"char\", ngram_range=(KMER_K, KMER_K), lowercase=False)\nX_seq = vect.fit_transform(X_text)\n\nparts_train = [X_seq]\n\n# Optional: taxonomy features\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tax_info = None\nif USE_TAXONOMY:\n    try:\n        train_tax = load_taxonomy(TRAIN_TAXONOMY)\n        test_tax  = load_taxonomy(TEST_TAXON)\n        dv = DictVectorizer(sparse=True)\n        X_tax_train = dv.fit_transform([{\"taxon\": train_tax.get(pid, \"NA\")} for pid in used_ids])\n        parts_train.append(X_tax_train)\n        tax_info = (test_tax, dv)\n        print(\"Taxonomy dims:\", X_tax_train.shape)\n    except Exception as e:\n        print(\"Taxonomy disabled:\", e)\n        tax_info = None\n\nX_train = parts_train[0] if len(parts_train) == 1 else sparse.hstack(parts_train).tocsr()\nprint(\"X_train shape:\", X_train.shape)\n\n# Labels (multi-label -> OvR)\nmlb = MultiLabelBinarizer()\nY_train = mlb.fit_transform(y_filtered)\nclasses = list(mlb.classes_)\nprint(\"Final class count:\", len(classes))\nif Y_train.shape[1] == 0:\n    raise RuntimeError(\"No GO classes after filtering. Relax MIN_COUNT and/or raise MAX_TERMS.\")\n\n# -------------------- TRAIN OvR XGBoost --------------------\nxgb_base_params = dict(\n    objective=\"binary:logistic\",\n    eval_metric=\"auc\",\n    max_depth=MAX_DEPTH,\n    eta=ETA,\n    subsample=SUBSAMPLE,\n    colsample_bytree=COLSAMPLE_BYTREE,\n    reg_lambda=LAMBDA_L2,\n    **dev_params,\n)\n\nmodels = []\nkept_class_indices = []  # indices of trained classes (some may be skipped)\nskipped = 0\n\nfor k in range(Y_train.shape[1]):\n    y = Y_train[:, k].astype(np.float32)\n    pos = int(y.sum())\n    if pos < MIN_POSITIVES_PER_CLASS:\n        models.append(None)\n        skipped += 1\n        continue\n    bst = train_xgb_gpu_binary(\n        X_train, y, xgb_base_params,\n        rounds=XGB_ROUNDS,\n        early_stopping=EARLY_STOPPING_ROUNDS,\n        val_size=VAL_SIZE,\n        seed=RANDOM_STATE\n    )\n    models.append(bst)\n    kept_class_indices.append(k)\n\nprint(f\"Trained {len(kept_class_indices)} classes; skipped {skipped} with < {MIN_POSITIVES_PER_CLASS} positives.\")\n\n# -------------------- BUILD TEST FEATURES --------------------\nprint(\"Loading test FASTA...\")\ntest_ids_full, test_seqs_full = read_fasta_ids_and_seqs(TEST_FASTA)\nif DEBUG_SMALL:\n    test_ids = test_ids_full[:DEBUG_N_TEST]\n    test_seqs = test_seqs_full[:DEBUG_N_TEST]\nelse:\n    test_ids, test_seqs = test_ids_full, test_seqs_full\n\nX_test_seq = vect.transform(test_seqs)\nparts_test = [X_test_seq]\n\nif tax_info:\n    test_tax, dv = tax_info\n    X_tax_test = dv.transform([{\"taxon\": test_tax.get(pid, \"NA\")} for pid in test_ids])\n    parts_test.append(X_tax_test)\n\nX_test = parts_test[0] if len(parts_test) == 1 else sparse.hstack(parts_test).tocsr()\nprint(\"X_test shape:\", X_test.shape)\n\n# -------------------- PREDICT --------------------\nprint(\"Predicting probabilities with XGBoost (GPU if available)...\")\ndtest = xgb.DMatrix(X_test)\nproba = np.zeros((X_test.shape[0], Y_train.shape[1]), dtype=np.float32)\n\nfor k, bst in enumerate(models):\n    if bst is None:\n        continue\n    # use best_ntree_limit if available\n    best_ntree = getattr(bst, \"best_ntree_limit\", 0)\n    if best_ntree and best_ntree > 0:\n        proba[:, k] = bst.predict(dtest, ntree_limit=best_ntree)\n    else:\n        proba[:, k] = bst.predict(dtest)\n\n# -------------------- WRITE SUBMISSION --------------------\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Formatting submission rows...\")\nrows = make_submission_rows(\n    pids=test_ids,\n    gids=classes,\n    proba=proba,\n    min_prob=MIN_PROB,\n    top_k=TOP_K,\n    cap=CAP_PER_PROT\n)\n\nprint(f\"Writing {len(rows)} rows -> {OUT_BASE}\")\nwith open(OUT_BASE, \"w\", encoding=\"utf-8\") as f:\n    for pid, go, sc in rows:\n        f.write(f\"{pid}\\t{go}\\t{sc}\\n\")\n\n# Auto-copy for Kaggle Submission\nshutil.copy(OUT_BASE, OUT_KAGGLE)\nprint(\"âœ… submission.tsv ready at:\", OUT_KAGGLE)\n\n# Preview\n!head -n 5 /kaggle/working/submission.tsv || true\n!wc -l /kaggle/working/submission.tsv || true","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}